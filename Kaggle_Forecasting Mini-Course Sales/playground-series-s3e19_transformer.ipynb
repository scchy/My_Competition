{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "file_root = Path('/home/scc/Downloads/playData')\n",
    "\n",
    "        \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(seed = 6666):\n",
    "    \"\"\"\n",
    "    设置随机种子\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # CPU\n",
    "    torch.manual_seed(seed) \n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed) \n",
    "    # python 全局\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "    # cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    print(f'Set env random_seed = {seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set env random_seed = 2023\n"
     ]
    }
   ],
   "source": [
    "all_seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365.0\n",
      "5.002739726027397\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Improve Your Coding</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Train More LLMs</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Win Friends and Influence People</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Win More Kaggle Competitions</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Write Better</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        date    country         store  \\\n",
       "0   0  2017-01-01  Argentina  Kaggle Learn   \n",
       "1   1  2017-01-01  Argentina  Kaggle Learn   \n",
       "2   2  2017-01-01  Argentina  Kaggle Learn   \n",
       "3   3  2017-01-01  Argentina  Kaggle Learn   \n",
       "4   4  2017-01-01  Argentina  Kaggle Learn   \n",
       "\n",
       "                                          product  num_sold  \n",
       "0               Using LLMs to Improve Your Coding        63  \n",
       "1                   Using LLMs to Train More LLMs        66  \n",
       "2  Using LLMs to Win Friends and Influence People         9  \n",
       "3      Using LLMs to Win More Kaggle Competitions        59  \n",
       "4                      Using LLMs to Write Better        49  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136950</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Improve Your Coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136951</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Train More LLMs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136952</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Win Friends and Influence People</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136953</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Win More Kaggle Competitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136954</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Write Better</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id        date    country         store  \\\n",
       "0  136950  2022-01-01  Argentina  Kaggle Learn   \n",
       "1  136951  2022-01-01  Argentina  Kaggle Learn   \n",
       "2  136952  2022-01-01  Argentina  Kaggle Learn   \n",
       "3  136953  2022-01-01  Argentina  Kaggle Learn   \n",
       "4  136954  2022-01-01  Argentina  Kaggle Learn   \n",
       "\n",
       "                                          product  \n",
       "0               Using LLMs to Improve Your Coding  \n",
       "1                   Using LLMs to Train More LLMs  \n",
       "2  Using LLMs to Win Friends and Influence People  \n",
       "3      Using LLMs to Win More Kaggle Competitions  \n",
       "4                      Using LLMs to Write Better  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df = pd.read_csv(file_root.joinpath('train.csv'))\n",
    "te_df = pd.read_csv(file_root.joinpath('test.csv'))\n",
    "print(te_df.shape[0]/5/3/5)\n",
    "print(tr_df.shape[0]/5/3/5/365)\n",
    "print(tr_df.num_sold.min())\n",
    "display(tr_df.head())\n",
    "te_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Argentina': 0, 'Canada': 1, 'Estonia': 2, 'Japan': 3, 'Spain': 4} \n",
      " {'Kagglazon': 0, 'Kaggle Learn': 1, 'Kaggle Store': 2} \n",
      " {'Using LLMs to Improve Your Coding': 0, 'Using LLMs to Train More LLMs': 1, 'Using LLMs to Win Friends and Influence People': 2, 'Using LLMs to Win More Kaggle Competitions': 3, 'Using LLMs to Write Better': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Argentina    27390\n",
       " Canada       27390\n",
       " Estonia      27390\n",
       " Japan        27390\n",
       " Spain        27390\n",
       " Name: country, dtype: int64,\n",
       " Kaggle Learn    45650\n",
       " Kaggle Store    45650\n",
       " Kagglazon       45650\n",
       " Name: store, dtype: int64,\n",
       " Using LLMs to Improve Your Coding                 27390\n",
       " Using LLMs to Train More LLMs                     27390\n",
       " Using LLMs to Win Friends and Influence People    27390\n",
       " Using LLMs to Win More Kaggle Competitions        27390\n",
       " Using LLMs to Write Better                        27390\n",
       " Name: product, dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_str2code = dict(zip(\n",
    "    sorted(tr_df.country.unique()),\n",
    "    range(tr_df.country.nunique())\n",
    "))\n",
    "store_str2code = dict(zip(\n",
    "    sorted(tr_df.store.unique()),\n",
    "    range(tr_df.store.nunique())\n",
    "))\n",
    "product_str2code = dict(zip(\n",
    "    sorted(tr_df['product'].unique()),\n",
    "    range(tr_df['product'].nunique())\n",
    "))\n",
    "\n",
    "print(country_str2code, '\\n', store_str2code, '\\n', product_str2code)\n",
    "tr_df.country.value_counts(), tr_df.store.value_counts(), tr_df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_LEN = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_slice(df, his_len=60, pred_len=7):\n",
    "    time_slices = sorted(df['date_time'].unique())\n",
    "    train_slice_list = []\n",
    "    for st, ed in zip(range(len(time_slices) - his_len), range(his_len - 1, len(time_slices) - pred_len)):\n",
    "        train_slice_list.append([time_slices[st], time_slices[ed], time_slices[ed + pred_len]])\n",
    "    return train_slice_list\n",
    "\n",
    "\n",
    "class regDataset(Dataset):\n",
    "    def __init__(self, pd_df, slice_list, his_len=60, pred_len=7):\n",
    "        super(regDataset, self).__init__()\n",
    "        self.his_len = his_len\n",
    "        self.pred_len = pred_len\n",
    "        self.df = pd_df.set_index('date_time')\n",
    "        self.slice_list = slice_list\n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.slice_list)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        sl = self.slice_list[0]\n",
    "        tr_sl = self.df.loc[sl[0]:sl[1], ['tp', 'tp_id', 'num_sold']].reset_index().sort_values(by=['date_time', 'tp'], ignore_index=True) \n",
    "        lb_sl = self.df.loc[sl[1]+1:sl[2], ['tp', 'tp_id', 'num_sold']].reset_index().sort_values(by=['date_time', 'tp'], ignore_index=True) \n",
    "        series = torch.Tensor(tr_sl[['num_sold']].values.reshape(-1, 75)).float()\n",
    "        labels = torch.Tensor(lb_sl[['num_sold']].values.reshape(-1, 75)).float()\n",
    "        \n",
    "        aa = tr_sl[['tp_id']].values.reshape(-1, 75)[0]\n",
    "        emb_ = np.stack([pd.DataFrame(aa)[0].str.split('&&', expand=True).values.astype(int) for _ in range(self.his_len)])\n",
    "        return torch.Tensor(emb_).long(), series, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df['country_id'] = tr_df['country'].map(country_str2code)\n",
    "tr_df['store_id'] = tr_df['store'].map(store_str2code)\n",
    "tr_df['product_id'] = tr_df['product'].map(product_str2code)\n",
    "tr_df['date_time'] = pd.to_datetime(tr_df['date'], format=\"%Y-%m-%d\")\n",
    "tr_df['tp_id'] = tr_df.country_id.map(str) + '&&'\\\n",
    "                + tr_df.store_id.map(str)   + \"&&\"\\\n",
    "                + tr_df['product_id'].map(str) + \"&&\"\\\n",
    "                + tr_df['date_time'].dt.day.map(str) + \"&&\"\\\n",
    "                + tr_df['date_time'].dt.dayofweek.map(str) \n",
    "tr_df['tp'] = tr_df.country + '&&' + tr_df.store  + \"&&\" + tr_df['product']\n",
    "total_slice_list = generate_time_slice(tr_df, pred_len=PRED_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(total_slice_list)\n",
    "tr_len = int(len(total_slice_list) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_slice = total_slice_list[:tr_len]\n",
    "val_slice = total_slice_list[tr_len:]\n",
    "tr_dataset = regDataset(tr_df, tr_slice, his_len=60, pred_len=PRED_LEN)\n",
    "val_dataset = regDataset(tr_df, val_slice, his_len=60, pred_len=PRED_LEN)\n",
    "\n",
    "tr_dataloader = DataLoader(tr_dataset, shuffle=True, batch_size=24)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60, 75, 5]), torch.Size([60, 75]), torch.Size([56, 75]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb, series_, label = tr_dataset[1]\n",
    "emb.shape, series_.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regNet(nn.Module):\n",
    "    def __init__(self, pred_len=7):\n",
    "        super(regNet, self).__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.city_emb = nn.Embedding(5, 4)\n",
    "        self.store_emb = nn.Embedding(3, 4)\n",
    "        self.product_emb = nn.Embedding(5, 8)\n",
    "        self.day_of_month_emb = nn.Embedding(32, 6)\n",
    "        self.day_of_week_emb = nn.Embedding(8, 6)\n",
    "        self.pos_layer_norm = nn.LayerNorm(12, eps=1e-12)\n",
    "\n",
    "        encoder_norm = nn.LayerNorm(75, eps=1e-5)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=75, nhead=5, norm_first=True, batch_first=True)\n",
    "        self.tf_encode = nn.TransformerEncoder(encoder_layer, num_layers=2, norm=encoder_norm)\n",
    "        \n",
    "        decode_norm = nn.LayerNorm(75, eps=1e-5)\n",
    "        decode_layer = nn.TransformerDecoderLayer(d_model=75, nhead=5, norm_first=True, batch_first=True)\n",
    "        self.tf_decode = nn.TransformerDecoder(decode_layer, num_layers=2, norm=decode_norm)\n",
    "        self.out_w = nn.Parameter(torch.randn(60, self.pred_len), requires_grad=True)\n",
    "        self.out_bais = nn.Parameter(torch.zeros(self.pred_len), requires_grad=True)\n",
    "\n",
    "    def forward(self, emb_ipt, seires_ipt):\n",
    "        # emb_ipt [b, 60, 75, 3]\n",
    "        city_res, store_res, product_res = [], [], []\n",
    "        day_res, week_res = [], []\n",
    "        batch_size = emb_ipt.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            city_res.append(self.city_emb(emb_ipt[i, :, :, 0])[:, 0, :])\n",
    "            store_res.append(self.store_emb(emb_ipt[i, :, :, 1])[:, 0, :])\n",
    "            product_res.append(self.product_emb(emb_ipt[i, :, :, 2])[:, 0, :])\n",
    "            day_res.append(self.day_of_month_emb(emb_ipt[i, :, :, 3])[:, 0, :])\n",
    "            week_res.append(self.day_of_week_emb(emb_ipt[i, :, :, 4])[:, 0, :])\n",
    "        \n",
    "        city = torch.stack(city_res)\n",
    "        store = torch.stack(store_res)\n",
    "        product = torch.stack(product_res)\n",
    "        day = torch.stack(day_res)\n",
    "        week = torch.stack(week_res)\n",
    "        # [b, 60, 4+4+8] \n",
    "        enity_emb = torch.cat([city, store, product], dim=-1)\n",
    "        \n",
    "        pos_emb = self.pos_layer_norm(torch.cat([day, week], dim=-1))\n",
    "        seires_ipt = torch.einsum('ble,bln->bln', [pos_emb, seires_ipt])\n",
    "        # [b, 60, 75] -> [b, 60, 75]\n",
    "        encode_out = self.tf_encode(seires_ipt)\n",
    "        seq_len = encode_out.size(1)\n",
    "        b = encode_out.size(0)\n",
    "        # n, nhead, len, len\n",
    "        mask = torch.stack([torch.tril(torch.ones(seq_len, seq_len)) for i in range(5 * b)]).to(seires_ipt.device)\n",
    "        # print('encode_out.shape=', encode_out.shape, \", mask.shape=\", mask.shape)\n",
    "        decode_out = self.tf_decode(encode_out, encode_out[:, -1, :].unsqueeze(1), tgt_mask=mask)\n",
    "        # [b, 60, 75] -> [b, 60, 75] -> [b, 7, 75]\n",
    "        decode_out = torch.einsum('ble,bln->bln', [enity_emb, decode_out])\n",
    "        res = torch.einsum(\"bln,lp->bnp\", [decode_out, self.out_w]) + self.out_bais\n",
    "        return torch.relu(torch.einsum(\"bnp->bpn\", res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 75, 5]) torch.Size([60, 75]) torch.Size([56, 75])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56, 75])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "emb, series_, label = tr_dataset[1]\n",
    "print(emb.shape, series_.shape, label.shape)\n",
    "reg = regNet(PRED_LEN).to(device)\n",
    "reg(emb.unsqueeze(0).to(device), series_.unsqueeze(0).to(device)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(y_pred, y):\n",
    "    res = 2*(y - y_pred).abs() / (y.abs() + y_pred.abs() + 1e-5)\n",
    "    return res.sum() / torch.prod(torch.Tensor(np.array(res.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate']) \n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "    save_path = config['save_path']\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models')\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_record = []\n",
    "        mape_record = []\n",
    "        train_pbar = tqdm(train_loader, position=0, leave=True)\n",
    "        for emb, series_, y in train_pbar:\n",
    "            optimizer.zero_grad()             \n",
    "            emb, series_, y = emb.to(device), series_.to(device), y.to(device)  \n",
    "            pred = model(emb, series_)\n",
    "            loss = criterion(pred, y)\n",
    "            smape = SMAPE(pred.cpu().detach(), y.cpu().detach())\n",
    "            loss.backward()                   \n",
    "\n",
    "            optimizer.step()    \n",
    "            step += 1\n",
    "            l_ = loss.detach().item()\n",
    "            loss_record.append(l_)\n",
    "            mape_record.append(smape)\n",
    "            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "            train_pbar.set_postfix({'loss': f'{l_:.5f}', \"smape\" : f'{smape:.5f}'})\n",
    "        \n",
    "        scheduler.step()\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "        mean_train_mape = sum(mape_record)/len(mape_record)\n",
    "        # model.eval() # 设置模型为评估模式\n",
    "        loss_record = []\n",
    "        mape_record = []\n",
    "        for emb, series_, y in valid_loader:\n",
    "            emb, series_, y = emb.to(device), series_.to(device), y.to(device)  \n",
    "            with torch.no_grad():\n",
    "                pred = model(emb, series_)\n",
    "                loss = criterion(pred, y)\n",
    "                smape = SMAPE(pred.cpu().detach(), y.cpu().detach())\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "            mape_record.append(smape)\n",
    "            \n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)\n",
    "        mean_valid_mape = sum(mape_record)/len(mape_record)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train SMAPE: {mean_train_mape:.4f} Loss: {mean_train_loss:.5f}, Valid SMAPE: {mean_valid_mape:.4f} Loss: {mean_valid_loss:.5f}')\n",
    "\n",
    "        if mean_valid_loss < best_loss: # mean_valid_loss\n",
    "            best_loss = mean_valid_loss\n",
    "            torch.save(model.state_dict(), save_path) \n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else: \n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'seed': 6666,\n",
    "    'n_epochs': 20,      \n",
    "    'learning_rate': 2e-3,#1e-4,           \n",
    "    'early_stop': 300,\n",
    "    'save_path': './models/model.ckpt'\n",
    "}\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c558131c9c4b14a8e22e52d1bbb8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]: Train SMAPE: 1.7627 Loss: 47242.41619, Valid SMAPE: 1.5506 Loss: 52277.82448\n",
      "Saving model with loss 52277.824...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fea0c10e69427895252f61484e8c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]: Train SMAPE: 1.3220 Loss: 34065.22896, Valid SMAPE: 1.2096 Loss: 36192.95312\n",
      "Saving model with loss 36192.953...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b207e020a9448ab1d91d00594d6062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]: Train SMAPE: 1.1130 Loss: 26303.09714, Valid SMAPE: 1.1736 Loss: 32861.38490\n",
      "Saving model with loss 32861.385...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d745a60aaa724363b2cafa2474dd3c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]: Train SMAPE: 1.0787 Loss: 25684.40858, Valid SMAPE: 1.1685 Loss: 32757.00039\n",
      "Saving model with loss 32757.000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37642330e0b47e58c408d65d4391c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]: Train SMAPE: 1.0772 Loss: 25648.15252, Valid SMAPE: 1.1737 Loss: 32666.45859\n",
      "Saving model with loss 32666.459...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04e506339a14102bbaacfaa86e1970e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]: Train SMAPE: 1.0808 Loss: 25652.03194, Valid SMAPE: 1.1807 Loss: 32737.59818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbfe61560154e2bbac5767c4be9aa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]: Train SMAPE: 1.0784 Loss: 25642.57048, Valid SMAPE: 1.1920 Loss: 32795.92161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9abd7f9ea6f4010a746804d5f8ee3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20]: Train SMAPE: 1.0789 Loss: 25633.61582, Valid SMAPE: 1.1923 Loss: 32935.71745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116578be84e94fe89f68016f2916d64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20]: Train SMAPE: 1.0780 Loss: 25619.77515, Valid SMAPE: 1.1895 Loss: 32906.20729\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36b1cb1e2744affa81db92e1a5460d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20]: Train SMAPE: 1.0740 Loss: 25605.22300, Valid SMAPE: 1.1963 Loss: 32964.00339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218b4dd53bc0401b8e5b19c04eca963c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20]: Train SMAPE: 1.0712 Loss: 25591.57271, Valid SMAPE: 1.1929 Loss: 33036.81771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f24bbbf6d744ed28386818b13467a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20]: Train SMAPE: 1.0691 Loss: 25583.86451, Valid SMAPE: 1.1961 Loss: 33073.07474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d31bc210ce04a698d883706ca03dd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20]: Train SMAPE: 1.0681 Loss: 25579.34481, Valid SMAPE: 1.1935 Loss: 33120.10781\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763f5ac77adc4a67b627f687f7871a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20]: Train SMAPE: 1.0660 Loss: 25574.03941, Valid SMAPE: 1.1894 Loss: 33121.64271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5010554e84a34d7c87fc9cc04ff18204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20]: Train SMAPE: 1.0643 Loss: 25570.89261, Valid SMAPE: 1.1901 Loss: 33136.57266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec69dc442bb47668de2f17af74406cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20]: Train SMAPE: 1.0629 Loss: 25568.18795, Valid SMAPE: 1.1914 Loss: 33133.31120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d88430d11c4e3a98619cb7b1832fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20]: Train SMAPE: 1.0618 Loss: 25566.74883, Valid SMAPE: 1.1922 Loss: 33145.89245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11addf750654607a461197e1d6369d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20]: Train SMAPE: 1.0605 Loss: 25564.77724, Valid SMAPE: 1.1924 Loss: 33196.31823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707200289ac04400b3f3df65b53ef31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20]: Train SMAPE: 1.0593 Loss: 25563.58748, Valid SMAPE: 1.1941 Loss: 33224.69349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bac9514c994853a6c5f88dfb6bbbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20]: Train SMAPE: 1.0584 Loss: 25562.63949, Valid SMAPE: 1.1889 Loss: 33153.26875\n"
     ]
    }
   ],
   "source": [
    "reg = regNet(PRED_LEN)\n",
    "trainer(tr_dataloader, val_dataloader, reg, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_df['country_id'] = te_df['country'].map(country_str2code)\n",
    "te_df['store_id'] = te_df['store'].map(store_str2code)\n",
    "te_df['product_id'] = te_df['product'].map(product_str2code)\n",
    "te_df['date_time'] = pd.to_datetime(te_df['date'], format=\"%Y-%m-%d\")\n",
    "te_df['tp_id'] = te_df.country_id.map(str) + '&&'\\\n",
    "                + te_df.store_id.map(str)   + \"&&\"\\\n",
    "                + te_df['product_id'].map(str) + \"&&\"\\\n",
    "                + te_df['date_time'].dt.day.map(str) + \"&&\"\\\n",
    "                + te_df['date_time'].dt.dayofweek.map(str) \n",
    "te_df['tp'] = te_df.country + '&&' + te_df.store  + \"&&\" + te_df['product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df_cp = tr_df.copy(deep=True).set_index('date_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d23f7ac94b47d68178f2a31dfa3e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg = regNet(PRED_LEN)\n",
    "reg.load_state_dict(torch.load(config['save_path'], map_location='cpu'))\n",
    "reg.to(device)\n",
    "\n",
    "pred_statr_dt = te_df.date_time.min() - timedelta(days=60)\n",
    "tt_pred_df = pd.concat([tr_df_cp.loc[pred_statr_dt:, :], te_df.set_index('date_time')])\n",
    "tt_pred_df = tt_pred_df.reset_index()\n",
    "tt_pred_df['c'] = tt_pred_df['tp'] + \"&&\" + tt_pred_df['date_time'].dt.strftime('%Y%m%d')\n",
    "tt_pred_df = tt_pred_df.set_index('date_time')\n",
    "\n",
    "his_len = 60\n",
    "groupby_cols = ['date_time', 'tp', 'tp_id']\n",
    "forcast_df_final = pd.DataFrame(columns=['date_time', 'tp', 'tp_id', 'num_sold'])\n",
    "tqd_bar = tqdm(range(365 - PRED_LEN + 1))\n",
    "for d in tqd_bar:\n",
    "    pred_ipt_st = pred_statr_dt + timedelta(days=d)\n",
    "    tqd_bar.set_description(f'start_dt={pred_ipt_st}')\n",
    "    pred_ipt_ed = pred_ipt_st + timedelta(days=59)\n",
    "    pred_f_ed = pred_ipt_ed + timedelta(days=PRED_LEN)\n",
    "    if d:\n",
    "        # update num_sold\n",
    "        tmp_mg = tt_pred_df.reset_index().merge(forcast_df_final.rename(columns={\"num_sold\": 'pred'})[['c', 'pred']], how='left', on='c')\n",
    "        pred_bool = (~tmp_mg.pred.isna()) & (tmp_mg['date_time'] >= te_df.date_time.min())\n",
    "        tmp_mg.loc[pred_bool, 'num_sold'] = tmp_mg[pred_bool].apply(\n",
    "            lambda c: c['pred'] if np.isnan(c['num_sold']) else (c['num_sold']+ c['pred'])/2, axis=1)\n",
    "        tt_pred_df = tmp_mg.drop(columns='pred').set_index('date_time')\n",
    "    sl = [pred_ipt_st, pred_ipt_ed, pred_f_ed]\n",
    "    pred_ipt = tt_pred_df.loc[sl[0]:sl[1], ['tp', 'tp_id', 'num_sold']].reset_index().sort_values(by=['date_time', 'tp'], ignore_index=True)\n",
    "    aa = pred_ipt[['tp_id']].values.reshape(-1, 75)[0]\n",
    "    series = torch.Tensor(pred_ipt[['num_sold']].values.reshape(-1, 75)).float()\n",
    "    emb_ = np.stack([pd.DataFrame(aa)[0].str.split('&&', expand=True).values.astype(int) for _ in range(his_len)])\n",
    "    emb_ = torch.Tensor(emb_).long()\n",
    "    with torch.no_grad():\n",
    "        pred_res = reg(emb_.unsqueeze(0).to(device), series.unsqueeze(0).to(device))\n",
    "    pred_flatten = pred_res[0].T.reshape(PRED_LEN*75).cpu().detach().numpy()\n",
    "    forcast_df = tt_pred_df.loc[sl[1] + timedelta(1):sl[2], ['tp', 'tp_id', 'num_sold']].reset_index().sort_values(by=['date_time', 'tp'], ignore_index=True)\n",
    "    forcast_df['num_sold'] = pred_flatten\n",
    "    \n",
    "    forcast_df_final = pd.concat([forcast_df_final, forcast_df])\n",
    "    forcast_df_final = forcast_df_final.groupby(groupby_cols, as_index=False)['num_sold'].mean()\n",
    "    forcast_df_final['c'] = forcast_df_final['tp'] + \"&&\" + forcast_df_final['date_time'].dt.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(file_root.joinpath('sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,num_sold\n",
      "136950,259\n",
      "136951,0\n",
      "136952,219\n",
      "136953,207\n",
      "136954,0\n",
      "136955,0\n",
      "136956,0\n",
      "136957,248\n",
      "136958,218\n"
     ]
    }
   ],
   "source": [
    "te_df['c'] = te_df['tp'] + \"&&\" + te_df['date_time'].dt.strftime('%Y%m%d')\n",
    "sub_df = sub_df[['id']].merge(\n",
    "    te_df.merge(forcast_df_final[['c', 'num_sold']], how='left', on='c')[['id', 'num_sold']],\n",
    "    on = 'id'\n",
    ")\n",
    "\n",
    "sub_df['num_sold'] = np.round(sub_df['num_sold']).map(int)\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sccRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
